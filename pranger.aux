\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Poisson distribution}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Vandermonde matrix}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}a}{5}{subsection.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The original data points $(x,y)$ together with the constructed 19-th degree polynomial using the coefficients found by solving the system $Vc=y$ using the LU decomposition of the corresponding Vandermonde matrix are shown. It is clear that the polynomial does not go through the original data points, as it lies way above them. However, the shape of the polynomial does somewhat seem to follow the original data points. It might thus be the case that in particular the offset coefficient $c_0$ has a large error, causing the polynomial to be shifted this way. The lower panel shows the absolute difference between the polynomial and the data points. As we can see, this offset is about 400 which is very large.}}{6}{figure.1}\protected@file@percent }
\newlabel{fig:2a}{{1}{6}{The original data points $(x,y)$ together with the constructed 19-th degree polynomial using the coefficients found by solving the system $Vc=y$ using the LU decomposition of the corresponding Vandermonde matrix are shown. It is clear that the polynomial does not go through the original data points, as it lies way above them. However, the shape of the polynomial does somewhat seem to follow the original data points. It might thus be the case that in particular the offset coefficient $c_0$ has a large error, causing the polynomial to be shifted this way. The lower panel shows the absolute difference between the polynomial and the data points. As we can see, this offset is about 400 which is very large}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}b}{6}{subsection.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The previous resulting polynomial from LU decomposition is shown together with the Lagrange polynomial found using Neville's algorithm with M=20. We see that the Lagrange polynomial does closely follow the original data points, with a small offset shown in the lower panel of about $10^{-14}$. The offset even crashes down for some x values, indicating that the Lagrange polynomial very accurately follows the data points. This is to be expected, as Neville's algorithm uses direct linear interpolation on the data points, and when used on the data points themselves, the result is correct up to round off and machine error. The method using the Vandermonde matrix, however, is a more indirect way of computing the polynomial with more intermediate calculations, which does not compute the polynomial values directly but the polynomial coefficients first. As a small deviation in coefficient value can make a large difference in the polynomial values, this explains why the original LU-decomposition method gives polynomial values with a much larger error than the Lagrange polynomial from Neville's algorithm.}}{7}{figure.2}\protected@file@percent }
\newlabel{fig:2b}{{2}{7}{The previous resulting polynomial from LU decomposition is shown together with the Lagrange polynomial found using Neville's algorithm with M=20. We see that the Lagrange polynomial does closely follow the original data points, with a small offset shown in the lower panel of about $10^{-14}$. The offset even crashes down for some x values, indicating that the Lagrange polynomial very accurately follows the data points. This is to be expected, as Neville's algorithm uses direct linear interpolation on the data points, and when used on the data points themselves, the result is correct up to round off and machine error. The method using the Vandermonde matrix, however, is a more indirect way of computing the polynomial with more intermediate calculations, which does not compute the polynomial values directly but the polynomial coefficients first. As a small deviation in coefficient value can make a large difference in the polynomial values, this explains why the original LU-decomposition method gives polynomial values with a much larger error than the Lagrange polynomial from Neville's algorithm}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}c}{7}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}d}{8}{subsection.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Again, the results from a and b are shown, but the polynomials found using 1 or 10 iterative improvements on the coefficients $c$ are added. We see that the first iterative improvement already greatly improves the polynomial found with LU decomposition: it now follows the data points just as the Lagrange polynomial. However, we do see that the offset is still larger for the improved LU polynomial compared to Neville's Lagrange polynomial. As explained previously, this can be explained by the fact that Neville's algorithm directly computes the polynomial values, while the LU decomposition method computes the coefficients of the polynomial after which those are used to compute the values. This indirect way allows for more error to accumulate. Furthermore, we see that using 10 iterative improvements compared to only 1 does not really improve the result any more. Lastly, we note that all polynomials behave quite normally in the middle of the range of x data points, but more chaotic around the edges of the data points. For example, the LU decomposition polynomials do not even go through the last data point at all. It might be the case that the actual polynomial we are looking for actually behaves more chaotically around the edges, but this can also be a result of the way in which the polynomial is calculated. Especially in the case of Neville's algorithm, the edge points are different from the middle points, because there is no information on what is going on with the polynomial outside the edges.}}{9}{figure.3}\protected@file@percent }
\newlabel{fig:2c}{{3}{9}{Again, the results from a and b are shown, but the polynomials found using 1 or 10 iterative improvements on the coefficients $c$ are added. We see that the first iterative improvement already greatly improves the polynomial found with LU decomposition: it now follows the data points just as the Lagrange polynomial. However, we do see that the offset is still larger for the improved LU polynomial compared to Neville's Lagrange polynomial. As explained previously, this can be explained by the fact that Neville's algorithm directly computes the polynomial values, while the LU decomposition method computes the coefficients of the polynomial after which those are used to compute the values. This indirect way allows for more error to accumulate. Furthermore, we see that using 10 iterative improvements compared to only 1 does not really improve the result any more. Lastly, we note that all polynomials behave quite normally in the middle of the range of x data points, but more chaotic around the edges of the data points. For example, the LU decomposition polynomials do not even go through the last data point at all. It might be the case that the actual polynomial we are looking for actually behaves more chaotically around the edges, but this can also be a result of the way in which the polynomial is calculated. Especially in the case of Neville's algorithm, the edge points are different from the middle points, because there is no information on what is going on with the polynomial outside the edges}{figure.3}{}}
